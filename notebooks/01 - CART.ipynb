{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CART / Decision Trees\r\n",
    "\r\n",
    "Our first notebook will look at the Classification and Regression Trees (CART) implementation in the `scikit-learn` library.\r\n",
    "\r\n",
    "CART is a fancy way of saying \"a decision tree,\" with some work happening behind the scenes to find the best solution given available information.  In practice, this generates a series of if-else statements which lead us to a single decision.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn import tree\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "clf = tree.DecisionTreeClassifier()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data\r\n",
    "\r\n",
    "For this demo, we will load a dataset of individuals and whether they have a high chance of heart attack (output = 1)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "heart_attack_data = \"../data/HeartAttackData.csv\"\r\n",
    "df = pd.read_csv(heart_attack_data, header=0)\r\n",
    "\r\n",
    "# Review the data\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "These measures aren't very self-explanatory, so let's explain them here.\r\n",
    "\r\n",
    "- `age` = Age of patient\r\n",
    "- `sex` = Sex of the patient (0 = female, 1 = male)\r\n",
    "- `cp` = Type of chest pain.\r\n",
    "  - 1 = Typical angina\r\n",
    "  - 2 = Atypical angina\r\n",
    "  - 3 = Non-anginal pain\r\n",
    "  - 4 = Asymptomatic\r\n",
    "- `trtbps` = Resting blood pressure (mm/Hg)\r\n",
    "- `chol` = Cholesterol level\r\n",
    "- `fbs` = Fasting blood sugar above 120 mg/dl\r\n",
    "- `restecg` = Resting ECG result\r\n",
    "  - 0 = Normal\r\n",
    "  - 1 = ST-T wave abnormality\r\n",
    "  - 2 = Probable or definite left ventricular hypertrophy\r\n",
    "- `thalachh` = Maximum heart rate achieved\r\n",
    "- `exng` = Exercise-induced angina (1 = yes, 0 = no)\r\n",
    "- `oldpeak` = Previous peak\r\n",
    "- `slp` = Slope\r\n",
    "- `caa` = Number of major vessels (0-3)\r\n",
    "- `thall` = Thalium Stress Test result (ranges from 0-3)\r\n",
    "- `output` = Diagnosis of heart disease (0 = < 50% diameter narrowing, 1 = > 50% diameter narrowing)\r\n",
    "\r\n",
    "Let's double-check the distinct values for the `output` feature:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[['output']].drop_duplicates()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a quick note, the implementation of CART that scikit-learn (sklearn) uses requires all inputs be numeric features.  Fortunately, this dataset happens to include only numeric features, so we don't need to do any special processing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Labels from Features\r\n",
    "\r\n",
    "Let's now create two variables:  `y`, which is the thing we want to predict (output: `{ 0, 1 }`); and `X`, which is everything we can use to predict the specific value of `y`.\r\n",
    "\r\n",
    "With Python, splitting data out like this will not shuffle the results (something we might have to worry about if we split the data up in SQL)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = df['output']\r\n",
    "X = df.drop('output', axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split into Training & Test Datasets\r\n",
    "\r\n",
    "The sklearn library has a method called `train_test_split` which breaks our data out into training and test datasets.  This allows us to train a model on one set of data and then see how it would perform on a completely different set of data.  This gives us a better idea of how our model might perform than simply using accuracy from the test dataset, as models tend to **overfit**:  they latch on the peculiarities of the training dataset.  If those peculiarities do not also exist in the broader population, then the trained model may come up with the wrong answer.  Having a separate test dataset that the trained model knows nothing about gives us a better idea of realistic behavior.  It also allows us to come up with a measure of how much overfitting the trained model does, as we can compare the training accuracy to the test accuracy; if there is a substantial difference between the two, our model is overfitting quite a bit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1740)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform Classification\r\n",
    "\r\n",
    "We'll train the model on our training data, ignoring the test data for now.  With sklearn, this is easy:  use the `fit()` method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = clf.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How'd we do?\r\n",
    "\r\n",
    "Let's use the `accuracy_score` method in sklearn to see just how well we did."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predicted = clf.predict(X_test)\r\n",
    "accuracy_score(y_test, predicted)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks like we predicted the correct answer about 75.8% of the time.  Not too bad for a few lines of work!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Viewing the Tree\r\n",
    "\r\n",
    "Decision trees are a great starting point for us because they are intuitive and we can visualize them easily.  Let's use a different library to visualize our resulting tree and see what factors the algorithm focused in on.\r\n",
    "\r\n",
    "In order to run this section, you will need the `graphviz` library.  You can get it from pip or conda:\r\n",
    "\r\n",
    "`conda install python-graphviz`\r\n",
    "\r\n",
    "`pip install graphviz`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import graphviz\r\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=X.columns.values.tolist(), class_names=['No heart attack', 'Heart attack'], filled=True, rounded=True, special_characters=True)\r\n",
    "graph = graphviz.Source(dot_data)\r\n",
    "graph"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Leaf nodes in **blue** were cases where there were heart attacks, and leaf nodes in **orange** had no heart attacks.  Non-leaf nodes are colored based on how likely they are to indicate a heart attack, with the starting point in our analysis being `thall <= 2.5`, as that seems to split things fairly well by itself."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}