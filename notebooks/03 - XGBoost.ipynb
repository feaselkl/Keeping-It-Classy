{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# XGBoost (Extreme Gradient Boosted Decision Trees)\r\n",
    "\r\n",
    "Our next notebook will look at a popular library called `XGBoost`, which stands for \"Extreme Gradient Boosting\" and applies to CART.  With random forests, we take a large number of decision trees and agglomerate their answers.  XGBoost starts from the same premise:  create an ensemble of trees.  The key difference is that XGBoost uses a technique called \"boosting\" to correct the mistakes of prior iterations in the sequence.\r\n",
    "\r\n",
    "In other words, we build the first model off of training data and see how it does.  Then, the second model takes the first model's results as an input and tries to correct what the first model got wrong.  Then, the next third model tries to improve upon the second model, and so on until either all training predictions are correct or we have reached a maximum number of classifier models (i.e., a maximum number of iterations).\r\n",
    "\r\n",
    "If you do not already have the package, install it with `conda install xgboost` or `pip install xgboost`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "import xgboost as xgb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data\r\n",
    "\r\n",
    "For this demo, we will load a dataset of individuals and whether they have a high chance of heart attack (output = 1)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "heart_attack_data = \"../data/HeartAttackData.csv\"\r\n",
    "df = pd.read_csv(heart_attack_data, header=0)\r\n",
    "\r\n",
    "# Review the data\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "These measures aren't very self-explanatory, so let's explain them here.  These are the same explanations that we saw in the CART notebook, but they're included again here for clarity.\r\n",
    "\r\n",
    "- `age` = Age of patient\r\n",
    "- `sex` = Sex of the patient (0 = female, 1 = male)\r\n",
    "- `cp` = Type of chest pain.\r\n",
    "  - 1 = Typical angina\r\n",
    "  - 2 = Atypical angina\r\n",
    "  - 3 = Non-anginal pain\r\n",
    "  - 4 = Asymptomatic\r\n",
    "- `trtbps` = Resting blood pressure (mm/Hg)\r\n",
    "- `chol` = Cholesterol level\r\n",
    "- `fbs` = Fasting blood sugar above 120 mg/dl\r\n",
    "- `restecg` = Resting ECG result\r\n",
    "  - 0 = Normal\r\n",
    "  - 1 = ST-T wave abnormality\r\n",
    "  - 2 = Probable or definite left ventricular hypertrophy\r\n",
    "- `thalachh` = Maximum heart rate achieved\r\n",
    "- `exng` = Exercise-induced angina (1 = yes, 0 = no)\r\n",
    "- `oldpeak` = Previous peak\r\n",
    "- `slp` = Slope\r\n",
    "- `caa` = Number of major vessels (0-3)\r\n",
    "- `thall` = Thalium Stress Test result (ranges from 0-3)\r\n",
    "- `output` = Diagnosis of heart disease (0 = < 50% diameter narrowing, 1 = > 50% diameter narrowing)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Labels from Features\r\n",
    "\r\n",
    "Let's now create two variables:  `y`, which is the thing we want to predict (output: `{ 0, 1 }`); and `X`, which is everything we can use to predict the specific value of `y`.\r\n",
    "\r\n",
    "With Python, splitting data out like this will not shuffle the results (something we might have to worry about if we split the data up in SQL)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = df['output']\r\n",
    "X = df.drop('output', axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split into Training & Test Datasets\r\n",
    "\r\n",
    "The sklearn library has a method called `train_test_split` which breaks our data out into training and test datasets.  This allows us to train a model on one set of data and then see how it would perform on a completely different set of data.  This gives us a better idea of how our model might perform than simply using accuracy from the test dataset, as models tend to **overfit**:  they latch on the peculiarities of the training dataset.  If those peculiarities do not also exist in the broader population, then the trained model may come up with the wrong answer.  Having a separate test dataset that the trained model knows nothing about gives us a better idea of realistic behavior.  It also allows us to come up with a measure of how much overfitting the trained model does, as we can compare the training accuracy to the test accuracy; if there is a substantial difference between the two, our model is overfitting quite a bit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1740)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform Classification\r\n",
    "\r\n",
    "We'll train the model on our training data, ignoring the test data for now.  With sklearn, this is easy:  use the `fit()` method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf = xgb.XGBClassifier(max_depth=5, n_estimators=45, use_label_encoder=False, eval_metric='logloss')\r\n",
    "clf = clf.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How'd we do?\r\n",
    "\r\n",
    "Let's use the `accuracy_score` method in sklearn to see just how well we did."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predicted = clf.predict(X_test)\r\n",
    "accuracy_score(y_test, predicted)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our accuracy score is now 83.5%, whereas for CART, it was 75.8%.  This ties the random forest classifier that we saw in the prior notebook.\r\n",
    "\r\n",
    "In practice, XGBoost is typically one of the better-performing algorithms, although random forest can often give it a run for its money.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Viewing feature importance\r\n",
    "\r\n",
    "Instead of looking at trees, let's see a plot of feature importance.  This is built into the `XGBoost` library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xgb.plot_importance(clf, max_num_features=7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on our training dataset, the two most important factors in determining whether a person is likely to present with a heart attack is age and cholesterol level.  After that, it looks like the maximum heart rate achieved (`thalachh`) is the next most important factor.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}