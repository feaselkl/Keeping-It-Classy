{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ad9fb9",
   "metadata": {},
   "source": [
    "# LightGBM (Light Gradient-Boosting Machine)\n",
    "\n",
    "`XGBoost` is not the only gradient boosting algorithm out there. Microsoft Research release `LightGBM` around the same time as `XGBoost` and it serves a similar purpose: fast gradient boosting for classification and regression problems.\n",
    "\n",
    "If you do not already have the package, install it with `conda install lightgbm` or `pip install lightgbm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc083385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41075a5a",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "For this demo, we will load a dataset of individuals and whether they have a high chance of long-term heart disease (output = 1) after a heart attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae59aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_attack_data = \"../data/HeartAttackData.csv\"\n",
    "df = pd.read_csv(heart_attack_data, header=0)\n",
    "\n",
    "# Review the data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110eedc0",
   "metadata": {},
   "source": [
    "These measures aren't very self-explanatory, so let's explain them here.  These are the same explanations that we saw in the CART notebook, but they're included again here for clarity.\n",
    "\n",
    "- `age` = Age of patient\n",
    "- `sex` = Sex of the patient (0 = female, 1 = male)\n",
    "- `cp` = Type of chest pain.\n",
    "  - 1 = Typical angina\n",
    "  - 2 = Atypical angina\n",
    "  - 3 = Non-anginal pain\n",
    "  - 4 = Asymptomatic\n",
    "- `trtbps` = Resting blood pressure (mm/Hg)\n",
    "- `chol` = Cholesterol level\n",
    "- `fbs` = Fasting blood sugar above 120 mg/dl\n",
    "- `restecg` = Resting ECG result\n",
    "  - 0 = Normal\n",
    "  - 1 = ST-T wave abnormality\n",
    "  - 2 = Probable or definite left ventricular hypertrophy\n",
    "- `thalachh` = Maximum heart rate achieved\n",
    "- `exng` = Exercise-induced angina (1 = yes, 0 = no)\n",
    "- `oldpeak` = Previous peak\n",
    "- `slp` = Slope\n",
    "- `caa` = Number of major vessels (0-3)\n",
    "- `thall` = Thalium Stress Test result (ranges from 0-3)\n",
    "- `output` = Diagnosis of heart disease (0 = < 50% diameter narrowing, 1 = > 50% diameter narrowing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6172b18",
   "metadata": {},
   "source": [
    "## Split Labels from Features\n",
    "\n",
    "Let's now create two variables:  `y`, which is the thing we want to predict (output: `{ 0, 1 }`); and `X`, which is everything we can use to predict the specific value of `y`.\n",
    "\n",
    "With Python, splitting data out like this will not shuffle the results (something we might have to worry about if we split the data up in SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0700a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['output']\n",
    "X = df.drop('output', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534be32b",
   "metadata": {},
   "source": [
    "## Split into Training & Test Datasets\n",
    "\n",
    "The sklearn library has a method called `train_test_split` which breaks our data out into training and test datasets.  This allows us to train a model on one set of data and then see how it would perform on a completely different set of data.  This gives us a better idea of how our model might perform than simply using accuracy from the test dataset, as models tend to **overfit**:  they latch on the peculiarities of the training dataset.  If those peculiarities do not also exist in the broader population, then the trained model may come up with the wrong answer.  Having a separate test dataset that the trained model knows nothing about gives us a better idea of realistic behavior.  It also allows us to come up with a measure of how much overfitting the trained model does, as we can compare the training accuracy to the test accuracy; if there is a substantial difference between the two, our model is overfitting quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28431b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1740)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b1f4c",
   "metadata": {},
   "source": [
    "## Perform Classification\n",
    "\n",
    "We'll train the model on our training data, ignoring the test data for now.  With sklearn, this is easy:  use the `fit()` method.\n",
    "\n",
    "Note that you may see a large number of warnings about no further splits with positive gain. That's okay; we'll ignore it here. Reducing the number of leaves or number of estimators does reduce the number of warning messages, but it also negatively impacts performance. Increasing the number of leaves or estimators does not seem to improve performance beyond these parameters, so there's not much benefit to further messing around with these values for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf337c-4369-423a-a76f-64cc205f2cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(num_leaves=28, max_depth=5, n_estimators=45)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e7556",
   "metadata": {},
   "source": [
    "## How'd we do?\n",
    "\n",
    "Let's use the `accuracy_score` method in sklearn to see just how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423dc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test)\n",
    "accuracy_score(y_test, predicted.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d41c85",
   "metadata": {},
   "source": [
    "Our accuracy score is now 83.5%, whereas for CART, it was 75.8%.  This ties the random forest classifier that we saw in the prior notebook, as well as XGBoost.\n",
    "\n",
    "In practice, LightGBM competes directly with XGBoost, as both can be fast, efficient algorithms for getting high-quality results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaad915",
   "metadata": {},
   "source": [
    "## Viewing feature importance\n",
    "\n",
    "Instead of looking at trees, let's see a plot of feature importance.  This is built into the `LightGBM` library and behaves just like it does in `XGBoost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0bd744",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(clf, max_num_features=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f124534",
   "metadata": {},
   "source": [
    "Based on our training dataset, the three most important factors in determining whether a person is likely to present with long-term heart disease is age, cholesterol level, and maximum heart rate achieved (`thalachh`). This is quite similar to the outcome we saw from XGBoost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e46af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
