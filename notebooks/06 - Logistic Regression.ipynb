{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is not really a classification technique, but instead a regression technique that we can use to classify.  Logistic regression is a special case of linear regression, where we apply a Sigmoid function to a linear regression problem.  This ends up pushing values on a curve with the majority of values being near 0 or near 1.  Let's take a look at campus recruitment using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prep the data.  Because we'll do it the same way for each, we only need to do this once.  I'll also remove the bits where we analyze the data, as we've seen it enough times already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campus_data = \"../data/CampusRecruitment.csv\"\n",
    "df = pd.read_csv(campus_data, header=0)\n",
    "y = df['status']\n",
    "X = df.drop(['sl_no', 'status', 'salary'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "For this dataset, we want to use the columns leading up to `status` to determine if different college graduates were placed at a job.  Because the salary is determined by the placement status, we can't use it to predict if a new graduate will be placed, so we'll have to drop that column.  Note that if we were interested in doing a regression analysis, we could try to predict the salary given placement, but we're keeping it classy and sticking to classification algorithms only. Also, we'll drop the `sl_no` column because it's a unique student number.\n",
    "\n",
    "Unlike the heart attack dataset, this dataset includes non-numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many Observations Do We Need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick reminder, our rule of thumb for observation count is: `10 * (number of features) / (probability of least common occurrence)`. That's a little hard to work with for continuous variables, so we'd probably want to bin each of those features and calculate the probabilities from a histogram. To keep things simple, I'll focus on the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df[['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation']]:\n",
    "    print(df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each categorical variable, we have 2-3 entries. That's a good sign. Now let's see what the frequency of each is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('215 total entries')\n",
    "for col in df[['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation']]:\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `hsc_s` == \"Arts\" and `degree_t` == \"Others\" are the least frequent, at 11 out of 215 occurrences. That gives us a probability of ~5%.\n",
    "\n",
    "So ideally, we would have a minimum `(10 * 11) / 0.05` observations in our dataset, as we have 11 features and 5% minimum likelihood for our categorical features. That's a total of 2200 observations, so we're clearly well under the ideal and should give us some pause. But we'll still keep working through this analysis, even though we realize there can be a problem with insufficient data leading to a weaker model than we'd want.\n",
    "\n",
    "Also, we can speculate that the 11 School of Arts students are the 11 people with `degree_t` == \"Others\" and so, for the remaining 204 graduates, we seem to have a fairly good mixture. This means we may struggle a bit with School of Arts graduates, but we can have some hope that it won't spill over into the other sets of students, for whom we do have a good number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Before we can feed this data into the logistic regression algorithm (or pretty much any other classification or regression algorithm), we need to convert any text data into numeric data.  There are a few common techniques for encoding.  The technique we will use for our dataset is called one-hot encoding.  What it does is \"pivot\" the categorical data, so that each distinct categorical value gets its own feature.  For example, `gender` has two values, M and F.  One-hot encoding will create new new features, one for `gender=M` and one for `gender=F`.  We need to do this for each of the non-numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "# Get only categorical features\n",
    "X_object = X.select_dtypes('object')\n",
    "\n",
    "# Fit the input features to our encoder\n",
    "enc.fit(X_object)\n",
    "\n",
    "# Perform the transformation on our dataset\n",
    "codes = enc.transform(X_object).toarray()\n",
    "feature_names = enc.get_feature_names_out(['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation'])\n",
    "\n",
    "# Combine the new one-hot encoded features with our existing floats and everything is now numeric.\n",
    "X = pd.concat([X.select_dtypes(exclude='object'), pd.DataFrame(codes, columns=feature_names).astype(int)], axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` here shows that we have the same number of rows as before (215), but the number of columns went from 12 to 21. We started with 5 floating point features. To it, we added 16 values for categorical variables: 5 of our categorical variables had 2 values, and two categorical variables had 3 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast, I'm going to perform a simple label encoding of the `status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a model using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How'd We Do?\n",
    "\n",
    "Let's first use the `accuracy_score` method in sklearn to see just how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test)\n",
    "accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's review the confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how things are looking so far for us:\n",
    "\n",
    "|Case|kNN|XGBoost|Logistic Regression|\n",
    "|----|---|-------|-------------------|\n",
    "|True Negative|13|18|17|\n",
    "|False Positive|8|3|4|\n",
    "|False Negative|4|5|6|\n",
    "|True Positive|40|39|38|\n",
    "\n",
    "Compared to kNN, the logistic regression model was better in terms of identifying non-placements, and slightly worse at identifying placements. It did ever so slightly worse than XGBoost in both cases, though there's not enough data for me to be confident in that statement.\n",
    "\n",
    "All in all, logistic regression performed well here despite the paucity of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
