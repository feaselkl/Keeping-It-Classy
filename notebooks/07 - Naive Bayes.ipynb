{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "The Naive Bayes series of classifiers are not really Bayesian, but the process kind of looks that way if you squint hard enough.  I like this classifier a lot because it's a fast, simple classifier which does reasonably well on datasets.  You can usually do better than Naive Bayes with a tailor-made model, but for something quick and dirty to see if you meet a minimum baseline for a go/no-go decision, it does well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prep the data.  Because we'll do it the same way for each, we only need to do this once.  I'll also remove the bits where we analyze the data, as we've seen it enough times already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campus_data = \"../data/CampusRecruitment.csv\"\n",
    "df = pd.read_csv(campus_data, header=0)\n",
    "y = df['status']\n",
    "X = df.drop(['sl_no', 'status', 'salary'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "Something special about Naive Bayes is that we need to know the number of unique categories in each feature, and we have to do that before splitting into training and test data.  Otherwise, we can run into an error where a categorical value shows up for the first time in the test data.  We can use `nunique()` to get the number of unique values for each column, and then change it to an array for input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.nunique().array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember that this includes some continuous variables as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of approaches we could take here.\n",
    "\n",
    "1. Perform two Naive Bayes calculations, one with continuous variables and one with categorical variables. Multiply the results of the two together to get our final outcome.\n",
    "2. Convert our continuous variables to discrete variables. We have a five-number summary and can bucketize results that way.\n",
    "\n",
    "Either approach is reasonable, but I'll go with the second approach, using the `KBinsDiscretizer()` to bin the data. This is almost the opposite approach from kNN and logistic regression, where we needed to perform ordinal encoding on categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = preprocessing.KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform', subsample=None)\n",
    "\n",
    "# Get only continuous features\n",
    "X_continuous = X.select_dtypes('float64')\n",
    "\n",
    "# Fit the input features to our encoder\n",
    "est.fit(X_continuous)\n",
    "\n",
    "# Perform the transformation on our dataset\n",
    "codes = est.transform(X_continuous)\n",
    "feature_names = est.get_feature_names_out(['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p'])\n",
    "\n",
    "# Combine the new discretized features with our existing categorical features and everything is now categorical.\n",
    "X = pd.concat([X.select_dtypes(exclude='float64'), pd.DataFrame(codes, columns=feature_names).astype(int)], axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at one of the new columns, we can see it is no longer continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['ssc_p'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this seriously cuts down on the number of unique values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.nunique().array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the Categorical Naive Bayes classifier based on these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = X.nunique().array\n",
    "clf = CategoricalNB(alpha=1, min_categories = num_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can feed this data into the categorical Naive Bayes algorithm (or pretty much any other classification algorithm), we need to convert any text data into numeric data.  There are a few common techniques for encoding.  The technique we will use for our dataset is called ordinal encoding.  What it does is convert strings to ordinal values for encoding.  For example, `gender` has two values, M and F.  Ordinal encoding will set one of these to 0 and the other to 1.  We need to do this for each of the non-numeric features. For the features we've already ordinally encoded, nothing will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OrdinalEncoder()\n",
    "\n",
    "X = enc.fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` here shows that we have the same number of rows as before (215) as well as the same number of columns (12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast, I'm going to perform a simple label encoding of the `status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How'd We Do?\n",
    "\n",
    "Let's first use the `accuracy_score` method in sklearn to see just how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test)\n",
    "accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's review the confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the rest of the group:\n",
    "\n",
    "|Case|kNN|XGBoost|Logistic Regression|Naive Bayes|\n",
    "|----|---|-------|-------------------|-----------|\n",
    "|True Negative|13|18|17|15|\n",
    "|False Positive|8|3|4|6|\n",
    "|False Negative|4|5|6|5|\n",
    "|True Positive|40|39|38|39|\n",
    "\n",
    "Compared to kNN, Naive Bayes did a little better at indicating when a person would not be placed after graduation. It did slightly worse at picking out the students who would receive placements. Its accuracy is right in line with these other algorithms.\n",
    "\n",
    "As a side note, if we did not discretize our dataset and simply ran `CategoricalNB()` on our continuous data, the accuracy would drop by about 8%, leading us to rather different conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
