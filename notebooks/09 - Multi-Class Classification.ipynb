{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-Class Classification\r\n",
    "\r\n",
    "To this point, we have focused on two-class classification.  This is the most common style, but there are scenarios in which two-class classification is not sufficient.  In this library, we will try out a variety of classification algorithms and see how they perform against this multi-class dataset.  We'll also see what changes, if any, we need to make this work.\r\n",
    "\r\n",
    "## Prep Work\r\n",
    "\r\n",
    "Before importing any classifier libraries, let's load our helper libraries and import the dataset for review."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn import preprocessing\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\r\n",
    "import matplotlib as mpl\r\n",
    "\r\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's prep the data.  Because we'll do it the same way for each, we only need to do this once."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bean_data = \"../data/Dry_Bean_Dataset.csv\"\r\n",
    "df = pd.read_csv(bean_data, header=0)\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a clean dataset, as all of our input features are numeric.  The class itself is not numeric, but that's okay--we won't need to make any changes there."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing\r\n",
    "\r\n",
    "The only pre-processing we'll need to do for most algorithms is to create our `X` and `y` sets, split, and go.  Certain algorithms, such as XGBoost, require numeric labels, but we'll handle this as we go along."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = df['Class']\r\n",
    "X = df.drop(['Class'], axis=1)\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1740)\r\n",
    "\r\n",
    "X_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification Showdown\r\n",
    "\r\n",
    "Now we're going to try out all of these algorithms we've used in a proper showdown.  Because this is a more complex problem than some of the ones we've looked at, we'll end up seeing a larger variance in scores between algorithms.  Note that any conclusions we can draw are around *this* dataset, not necessarily an indictment of any algorithm in particular.\r\n",
    "\r\n",
    "### CART\r\n",
    "\r\n",
    "The first algorithm we'll look at is CART."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import tree\r\n",
    "clf_cart = tree.DecisionTreeClassifier()\r\n",
    "clf_cart = clf_cart.fit(X_train, y_train)\r\n",
    "predicted_cart = clf_cart.predict(X_test)\r\n",
    "accuracy_score(y_test, predicted_cart)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cm_cart = confusion_matrix(y_test, predicted_cart)\r\n",
    "ConfusionMatrixDisplay(cm_cart).plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(classification_report(y_test, predicted_cart))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CART does a great job with accuracy and is a solid answer across the board.\r\n",
    "\r\n",
    "### Random Forest\r\n",
    "\r\n",
    "Our next algorithm is random forest."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import ensemble\r\n",
    "clf_rf = ensemble.RandomForestClassifier()\r\n",
    "clf_rf = clf_rf.fit(X_train, y_train)\r\n",
    "predicted_rf = clf_rf.predict(X_test)\r\n",
    "accuracy_score(y_test, predicted_rf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cm_rf = confusion_matrix(y_test, predicted_rf)\r\n",
    "ConfusionMatrixDisplay(cm_rf).plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(classification_report(y_test, predicted_rf))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random forest, as we'd expect, does even better than CART.\r\n",
    "\r\n",
    "### XGBoost\r\n",
    "\r\n",
    "Our next algorithm is XGBoost.  XGBoost **does** require numerical labels, so we'll need to use encoded labels.  Previously, we've used the `LabelEncoder` to build this, but here we'll rig something simple."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rickety_encoder(grain):\r\n",
    "    if grain == \"BARBUNYA\":\r\n",
    "        return 0\r\n",
    "    elif grain == \"BOMBAY\":\r\n",
    "        return 1\r\n",
    "    elif grain == \"CALI\":\r\n",
    "        return 2\r\n",
    "    elif grain == \"DERMASON\":\r\n",
    "        return 3\r\n",
    "    elif grain == \"HOROZ\":\r\n",
    "        return 4\r\n",
    "    elif grain == \"SEKER\":\r\n",
    "        return 5\r\n",
    "    elif grain == \"SIRA\":\r\n",
    "        return 6\r\n",
    "\r\n",
    "y_train_enc = list(map(rickety_encoder, y_train))\r\n",
    "y_test_enc = list(map(rickety_encoder, y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've encoded our training and test labels, let's perform our tests."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xgboost as xgb\r\n",
    "clf_xgb = xgb.XGBClassifier(max_depth=5, n_estimators=45, use_label_encoder=False, eval_metric='logloss')\r\n",
    "clf_xgb = clf_xgb.fit(X_train, y_train_enc)\r\n",
    "predicted_xgb = clf_xgb.predict(X_test)\r\n",
    "accuracy_score(y_test_enc, predicted_xgb)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy is slightly better than random forest, but in order to see the same confusion matrix, we'll need to build another rickety conversion function to get back to where we were."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rickety_decoder(grain):\r\n",
    "    if grain == 0:\r\n",
    "        return \"BARBUNYA\"\r\n",
    "    elif grain == 1:\r\n",
    "        return \"BOMBAY\"\r\n",
    "    elif grain == 2:\r\n",
    "        return \"CALI\"\r\n",
    "    elif grain == 3:\r\n",
    "        return \"DERMASON\"\r\n",
    "    elif grain == 4:\r\n",
    "        return \"HOROZ\"\r\n",
    "    elif grain == 5:\r\n",
    "        return \"SEKER\"\r\n",
    "    elif grain == 6:\r\n",
    "        return \"SIRA\"\r\n",
    "\r\n",
    "predicted_xgb_dec = list(map(rickety_decoder, predicted_xgb))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's see how it performs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cm_xgb = confusion_matrix(y_test, predicted_xgb_dec)\r\n",
    "ConfusionMatrixDisplay(cm_xgb).plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(classification_report(y_test, predicted_xgb_dec))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results are extremely close to our random forest, but tend to be slightly better.  It looks like SIRA is the toughest to predict.\r\n",
    "\r\n",
    "### K-Nearest Neighbors\r\n",
    "\r\n",
    "Our next algorithm is k-Nearest Neighbors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5)\r\n",
    "clf_knn = clf_knn.fit(X_train, y_train)\r\n",
    "predicted_knn = clf_knn.predict(X_test)\r\n",
    "accuracy_score(y_test, predicted_knn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a pretty big drop-off from the tree-based algorithms, and the results are fairly stable for several values of k."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cm_knn = confusion_matrix(y_test, predicted_knn)\r\n",
    "ConfusionMatrixDisplay(cm_knn).plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(classification_report(y_test, predicted_knn))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that there's a big dropoff in almost everything except for Bombay.\r\n",
    "\r\n",
    "### Logistic Regression\r\n",
    "\r\n",
    "Our next algorithm is logistic regression."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\r\n",
    "clf_lr = LogisticRegression(max_iter=5000)\r\n",
    "clf_lr = clf_lr.fit(X_train, y_train)\r\n",
    "predicted_lr = clf_lr.predict(X_test)\r\n",
    "accuracy_score(y_test, predicted_lr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy is back on track.  Here, accuracy is worse than random forest or XGBoost but better than CART."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cm_lr = confusion_matrix(y_test, predicted_lr)\r\n",
    "ConfusionMatrixDisplay(cm_lr).plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(classification_report(y_test, predicted_lr))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logistic regression does a rather good job of it.  It doesn't do terribly on any of the classes, though like everything else, it has difficulties with Sira.\r\n",
    "\r\n",
    "### Naive Bayes\r\n",
    "\r\n",
    "Our next algorithm is multinomial Naive Bayes.  `CategoricalNB` is for cases where we have categorical input data, but all of our features are numeric.  Because of that, we can use a different variant of Naive Bayes.  The best-fit classifier for Naive Bayes in scikit-learn, in our case, is the multinomial classifier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "clf_nb = MultinomialNB(alpha=1)\r\n",
    "clf_nb = clf_nb.fit(X_train, y_train)\r\n",
    "predicted_nb = clf_nb.predict(X_test)\r\n",
    "accuracy_score(y_test, predicted_nb)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a little better than kNN, but way below the trees or logistic regression."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cm_nb = confusion_matrix(y_test, predicted_nb)\r\n",
    "ConfusionMatrixDisplay(cm_nb).plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(classification_report(y_test, predicted_nb))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bombay still appears to be easily separable, but for the rest of them, values are worse across the board.\r\n",
    "\r\n",
    "### Passive-Aggressive\r\n",
    "\r\n",
    "Our next algorithm is passive-aggressive classification."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\r\n",
    "\r\n",
    "clf_pa = PassiveAggressiveClassifier(C=1.0, max_iter=3000, random_state=1740)\r\n",
    "clf_pa = clf_pa.fit(X_train, y_train)\r\n",
    "predicted_pa = clf_pa.predict(X_test)\r\n",
    "accuracy_score(y_test, predicted_pa)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wow, what a drop-off!  Passive-Aggressive classifiers often give you great outcomes, but in this case, it's a dog."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cm_pa = confusion_matrix(y_test, predicted_pa)\r\n",
    "ConfusionMatrixDisplay(cm_pa).plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This visualization helps us see what's going wrong:  it looks like the passive-aggressive classifier has become so focused on Dermason that it completely overwhelms Horoz and Seker."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(classification_report(y_test, predicted_pa))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see the same pattern here as we've seen all along, where Bombay is easy and Sira is tough.  Barbunya typically isn't that tough, but passive-aggressive is struggling in our scenario.\r\n",
    "\r\n",
    "The moral of the story is, don't limit yourself to just one classifier!"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}